# 带遗忘因子的递推最小二乘（RLS with Forgetting Factor）

## 算法概述

带遗忘因子的递推最小二乘算法在标准RLS基础上引入了遗忘因子 $\alpha$（$0 < \alpha \leq 1$），其核心思想是使算法能够适应时变系统，通过指数加权衰减历史数据的影响，给新数据更大的权重，从而实现参数的跟踪能力。

## 问题描述

考虑线性模型：
$$
y(t) = \boldsymbol{\psi}^T(t) \boldsymbol{\theta} + e(t)
$$
其中：
- $\boldsymbol{\theta} \in \mathbb{R}^n$ 为待估计的参数向量
- $\boldsymbol{\psi}(t) \in \mathbb{R}^n$ 为在时刻 $t$ 的输入（回归）向量
- $y(t)$ 为时刻 $t$ 的输出观测值
- $e(t)$ 为建模误差或噪声

## 算法结论-带遗忘因子的RLS算法步骤

**初始化**
$$
\hat{\boldsymbol{\theta}}_0 = \boldsymbol{0}, \quad \boldsymbol{P}_0 = \delta \boldsymbol{I}, \quad \delta \text{为一个很大的正数（如}10^3 \sim 10^6\text{）}
$$
其中遗忘因子 $\alpha$ 通常取 $0.95 < \alpha \leq 1$，$\alpha$ 越小，遗忘速度越快。

---

**对每个时刻 $t=1,2,\dots$ 执行**

- **收集新数据**：获得 $\boldsymbol{\psi}(t)$ 和 $y(t)$
- **计算先验误差**：
  $$
  \epsilon(t) = y(t) - \boldsymbol{\psi}^T(t) \hat{\boldsymbol{\theta}}_{t-1}
  $$
- **计算增益向量**：
  $$
  \boldsymbol{k}_t = \frac{\boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)}{\alpha + \boldsymbol{\psi}^T(t) \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)}
  $$
- **更新参数估计**：
  $$
  \hat{\boldsymbol{\theta}}_t = \hat{\boldsymbol{\theta}}_{t-1} + \boldsymbol{k}_t \epsilon(t)
  $$
- **更新协方差矩阵**：
  $$
  \boldsymbol{P}_t = \frac{1}{\alpha} \left( \boldsymbol{P}_{t-1} - \boldsymbol{k}_t \boldsymbol{\psi}^T(t) \boldsymbol{P}_{t-1} \right)
  $$
- **准备下一时刻**：$t \leftarrow t+1$

**算法核心解释**：遗忘因子 $\alpha$ 的引入使得算法具有"记忆衰减"特性。当 $\alpha < 1$ 时，历史数据的权重随时间指数衰减 $(\alpha^{t-i})$，新数据对参数估计的影响更大，从而使算法能够跟踪时变参数。

---

## 带遗忘因子的RLS算法证明

**引入遗忘因子的代价函数**

标准RLS最小化所有历史数据的误差平方和，而带遗忘因子的RLS最小化指数加权平方和：
$$
J(\boldsymbol{\theta}) = \sum_{i=1}^{t} \alpha^{t-i} [y(i) - \boldsymbol{\psi}^T(i)\boldsymbol{\theta}]^2
$$
其中 $0 < \alpha \leq 1$ 为遗忘因子：
- $\alpha = 1$：退化为标准RLS（无遗忘）
- $\alpha < 1$：历史数据权重按指数衰减，$\alpha$ 越小衰减越快

---

**批量解的形式**

基于 $t$ 个观测数据 $\{\boldsymbol{\psi}(i), y(i)\}_{i=1}^{t}$，定义加权数据矩阵和输出向量：
$$
\boldsymbol{\Phi}_t = \begin{bmatrix}
\alpha^{(t-1)/2} & & \\
& \ddots & \\
& & \alpha^{0}
\end{bmatrix} \begin{bmatrix}
\boldsymbol{\psi}^T(1) \\
\vdots \\
\boldsymbol{\psi}^T(t)
\end{bmatrix}, \quad \boldsymbol{Y}_t = \begin{bmatrix}
\alpha^{(t-1)/2} & & \\
& \ddots & \\
& & \alpha^{0}
\end{bmatrix} \begin{bmatrix}
y(1) \\
\vdots \\
y(t)
\end{bmatrix}
$$

对应的正则方程为：
$$
\hat{\boldsymbol{\theta}}_t = (\boldsymbol{\Phi}_t^T\boldsymbol{\Phi}_t)^{-1} \boldsymbol{\Phi}_t^T \boldsymbol{Y}_t
$$

定义两个关键矩阵：
$$
\boldsymbol{P}_t^{-1} = \boldsymbol{\Phi}_t^T\boldsymbol{\Phi}_t = \sum_{i=1}^{t} \alpha^{t-i} \boldsymbol{\psi}(i)\boldsymbol{\psi}^T(i)
$$
$$
\boldsymbol{B}_t = \boldsymbol{\Phi}_t^T \boldsymbol{Y}_t = \sum_{i=1}^{t} \alpha^{t-i} \boldsymbol{\psi}(i) y(i)
$$

则批量解可写为：
$$
\hat{\boldsymbol{\theta}}_t = \boldsymbol{P}_t \boldsymbol{B}_t
$$

---

**$\boldsymbol{P}_t^{-1}$ 和 $\boldsymbol{B}_t$ 的递推**

根据定义，它们具有指数加权的递推形式：
$$
\boldsymbol{P}_t^{-1} = \alpha \boldsymbol{P}_{t-1}^{-1} + \boldsymbol{\psi}(t)\boldsymbol{\psi}^T(t)
$$
$$
\boldsymbol{B}_t = \alpha \boldsymbol{B}_{t-1} + \boldsymbol{\psi}(t) y(t)
$$

**推导**：
$$
\begin{aligned}
\boldsymbol{P}_t^{-1} &= \sum_{i=1}^{t} \alpha^{t-i} \boldsymbol{\psi}(i)\boldsymbol{\psi}^T(i) \\
&= \alpha \sum_{i=1}^{t-1} \alpha^{t-1-i} \boldsymbol{\psi}(i)\boldsymbol{\psi}^T(i) + \boldsymbol{\psi}(t)\boldsymbol{\psi}^T(t) \\
&= \alpha \boldsymbol{P}_{t-1}^{-1} + \boldsymbol{\psi}(t)\boldsymbol{\psi}^T(t)
\end{aligned}
$$
同理可得 $\boldsymbol{B}_t$ 的递推式。

---

$\boldsymbol{P}_t$ 的递推（应用矩阵求逆引理）

已知：
$$
\boldsymbol{P}_t^{-1} = \alpha \boldsymbol{P}_{t-1}^{-1} + \boldsymbol{\psi}(t)\boldsymbol{\psi}^T(t)
$$

根据**矩阵求逆引理（Woodbury Identity）**：
若 $\boldsymbol{A}^{-1} = \boldsymbol{B}^{-1} + \boldsymbol{C}\boldsymbol{D}^{-1}\boldsymbol{C}^T$，则
$\boldsymbol{A} = \boldsymbol{B} - \boldsymbol{B}\boldsymbol{C}(\boldsymbol{D} + \boldsymbol{C}^T\boldsymbol{B}\boldsymbol{C})^{-1}\boldsymbol{C}^T\boldsymbol{B}$。

令：
- $\boldsymbol{A} = \boldsymbol{P}_t$
- $\boldsymbol{B} = \frac{1}{\alpha} \boldsymbol{P}_{t-1}$
- $\boldsymbol{C} = \boldsymbol{\psi}(t)$
- $\boldsymbol{D} = 1$

代入得：
$$
\boldsymbol{P}_t = \frac{1}{\alpha} \boldsymbol{P}_{t-1} - \frac{1}{\alpha} \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t) \left[ 1 + \boldsymbol{\psi}^T(t) \frac{1}{\alpha} \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t) \right]^{-1} \boldsymbol{\psi}^T(t) \frac{1}{\alpha} \boldsymbol{P}_{t-1}
$$

化简得：
$$
\boldsymbol{P}_t = \frac{1}{\alpha} \boldsymbol{P}_{t-1} - \frac{1}{\alpha^2} \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t) \left[ 1 + \frac{1}{\alpha} \boldsymbol{\psi}^T(t) \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t) \right]^{-1} \boldsymbol{\psi}^T(t) \boldsymbol{P}_{t-1}
$$

定义**增益向量** $\boldsymbol{k}_t$：
$$
\boldsymbol{k}_t = \frac{\boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)}{\alpha + \boldsymbol{\psi}^T(t) \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)}
$$

则 $\boldsymbol{P}_t$ 的更新公式简化为：
$$
\boldsymbol{P}_t = \frac{1}{\alpha} \left( \boldsymbol{P}_{t-1} - \boldsymbol{k}_t \boldsymbol{\psi}^T(t) \boldsymbol{P}_{t-1} \right)
$$

---

**$\hat{\boldsymbol{\theta}}_t$ 的递推**

由批量解 $\hat{\boldsymbol{\theta}}_t = \boldsymbol{P}_t \boldsymbol{B}_t$ 出发，代入 $\boldsymbol{B}_t$ 和 $\boldsymbol{P}_t$ 的递推式：
$$
\begin{aligned}
\hat{\boldsymbol{\theta}}_t &= \boldsymbol{P}_t \boldsymbol{B}_t \\
&= \boldsymbol{P}_t \left[ \alpha \boldsymbol{B}_{t-1} + \boldsymbol{\psi}(t) y(t) \right] \\
&= \alpha \boldsymbol{P}_t \boldsymbol{B}_{t-1} + \boldsymbol{P}_t \boldsymbol{\psi}(t) y(t)
\end{aligned}
$$

由 $\hat{\boldsymbol{\theta}}_{t-1} = \boldsymbol{P}_{t-1} \boldsymbol{B}_{t-1}$ 得 $\boldsymbol{B}_{t-1} = \boldsymbol{P}_{t-1}^{-1} \hat{\boldsymbol{\theta}}_{t-1}$，代入：
$$
\hat{\boldsymbol{\theta}}_t = \alpha \boldsymbol{P}_t \boldsymbol{P}_{t-1}^{-1} \hat{\boldsymbol{\theta}}_{t-1} + \boldsymbol{P}_t \boldsymbol{\psi}(t) y(t)
$$

由 $\boldsymbol{P}_t^{-1} = \alpha \boldsymbol{P}_{t-1}^{-1} + \boldsymbol{\psi}(t)\boldsymbol{\psi}^T(t)$，得：
$$
\alpha \boldsymbol{P}_{t-1}^{-1} = \boldsymbol{P}_t^{-1} - \boldsymbol{\psi}(t)\boldsymbol{\psi}^T(t)
$$

代入 $\alpha \boldsymbol{P}_t \boldsymbol{P}_{t-1}^{-1} = \boldsymbol{P}_t \left( \boldsymbol{P}_t^{-1} - \boldsymbol{\psi}(t)\boldsymbol{\psi}^T(t) \right) = \boldsymbol{I} - \boldsymbol{P}_t \boldsymbol{\psi}(t)\boldsymbol{\psi}^T(t)$

因此：
$$
\hat{\boldsymbol{\theta}}_t = \left[ \boldsymbol{I} - \boldsymbol{P}_t \boldsymbol{\psi}(t)\boldsymbol{\psi}^T(t) \right] \hat{\boldsymbol{\theta}}_{t-1} + \boldsymbol{P}_t \boldsymbol{\psi}(t) y(t)
$$

整理得：
$$
\hat{\boldsymbol{\theta}}_t = \hat{\boldsymbol{\theta}}_{t-1} + \boldsymbol{P}_t \boldsymbol{\psi}(t) \left[ y(t) - \boldsymbol{\psi}^T(t) \hat{\boldsymbol{\theta}}_{t-1} \right]
$$

验证 $\boldsymbol{P}_t \boldsymbol{\psi}(t) = \boldsymbol{k}_t$：
$$
\begin{aligned}
\boldsymbol{P}_t \boldsymbol{\psi}(t) &= \frac{1}{\alpha} \left( \boldsymbol{P}_{t-1} - \boldsymbol{k}_t \boldsymbol{\psi}^T(t) \boldsymbol{P}_{t-1} \right) \boldsymbol{\psi}(t) \\
&= \frac{1}{\alpha} \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t) - \frac{1}{\alpha} \boldsymbol{k}_t \boldsymbol{\psi}^T(t) \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)
\end{aligned}
$$

将 $\boldsymbol{k}_t = \frac{\boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)}{\alpha + \boldsymbol{\psi}^T(t) \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)}$ 代入，令 $s = \boldsymbol{\psi}^T(t) \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)$：
$$
\begin{aligned}
\boldsymbol{P}_t \boldsymbol{\psi}(t) &= \frac{1}{\alpha} \boldsymbol{P}_{t-1} \boldsymbol{\psi}(t) - \frac{1}{\alpha} \cdot \frac{\boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)}{\alpha + s} \cdot s \\
&= \frac{\boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)}{\alpha} \left( 1 - \frac{s}{\alpha + s} \right) \\
&= \frac{\boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)}{\alpha} \cdot \frac{\alpha}{\alpha + s} \\
&= \frac{\boldsymbol{P}_{t-1} \boldsymbol{\psi}(t)}{\alpha + s} = \boldsymbol{k}_t
\end{aligned}
$$

因此，参数更新公式为：
$$
\hat{\boldsymbol{\theta}}_t = \hat{\boldsymbol{\theta}}_{t-1} + \boldsymbol{k}_t \epsilon(t)
$$
其中 $\epsilon(t) = y(t) - \boldsymbol{\psi}^T(t) \hat{\boldsymbol{\theta}}_{t-1}$ 为先验误差。

---

**遗忘因子的选择**

- **$\alpha = 1$**：标准RLS，无遗忘，适用于参数恒定的系统
- **$0.95 < \alpha < 1$**：常用范围，适用于缓慢时变系统
- **$\alpha \leq 0.95$**：快速遗忘，适用于快速时变系统，但可能导致估计方差增大
- **经验公式**：$\alpha = 1 - \frac{1}{N}$，其中 $N$ 为有效记忆长度

**算法特性**

1. **跟踪能力**：$\alpha < 1$ 时，算法能跟踪时变参数
2. **数值稳定性**：$\alpha$ 的引入有助于防止 $\boldsymbol{P}_t$ 趋于零，保持算法的灵敏度
3. **折衷设计**：$\alpha$ 的选择需要在跟踪速度（小$\alpha$）和估计精度（大$\alpha$）之间权衡
